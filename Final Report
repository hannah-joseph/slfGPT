---
title: "Comparative Analysis of Three Generative AI Models in Spotted Lanternfly Research"
author: "Hannah Joseph"
date: "`r Sys.Date()`"
format: 
  html: 
    self-contained: true
    fig_path: "figures/"
    fig-width: 6
    fig-asp: 0.618
    toc: true
    toc-location: right
    theme: journal
editor: visual
execute:
  echo: false
bibliography: references.bib
---

# [Aim]{.underline}

Generative Pre-trained Transformers (GPT) have opened new ways in utilizing artificial intelligence as a tool in scientific research. GPT models are a type of Large Language Models (LLM) that utilizes a type of neural network architecture called transformer, used to process, generate, and learn data from examples and generate a prompt based on users' inquiries [@ooi2023]. The development of a GPT specialized for Spotted Lanternfly (*LycormaÂ delicatula*) research addresses the need for comprehensive and accessible knowledge about this invasive species. Native to China, Japan, and Vietnam, the spotted lanternfly is a polyphagous invasive species that was first detected in Berks County, Pennsylvania in September 2014, and became a significant pest of various ornamental plants and vineyards [@urban2023]. To address this need, the GPT contains a library of information, known as the knowledge base, compiled of peer-reviewed research papers, guidelines, and websites about the biology, behavior, ecological impact, and control measures of this species. In addition, prompt engineering, a process of designing prompts or inputs to generative AI models to elicit a specific response was used to develop the knowledge base [@fui-hoonnah2023]. With this, it provides users with detailed information, making scientific knowledge accessible to a broader audience, and to practice more effective management practices, contributing to mitigate the impacts of this invasive species.

# [Question]{.underline}

The main question I would like to address is:

-   How well does a GPT generate responses with a structured knowledge base, compared to a GPT with no knowledge base?

    Additional questions include:

-   Does the presence of a knowledge base create a significant difference in the responses that a GPT generates?

-   Does prompt engineering create a significant difference in the responses that a GPT generates?

# [Scope and System]{.underline}

The domain of my research encompasses the use of three generative pre-trained transformer (GPT) models specialized in research of spotted lanternflies. GPT 1 and GPT 2 were trained through prompt engineering to answer inquiries about spotted lanternfly research:

-   SLF-GPT, (known as GPT 1), composes of a knowledge base containing scientific research papers about spotted lanternflies and a focus in spotted lanternfly research.

-   EntomoResearcher, (known as GPT 2), does not contain a knowledge base but has a focus in spotted lanternfly research.

-   Lastly, ChatGPT, (known as GPT 3) has no knowledge base or focus in spotted lanternfly research.

    The system is in a controlled environment and will function based on specific queries about spotted lanternflies (e.g. biology, history/origins, management, control, environmental impacts, etc.). The fundamental units of analysis is based on the responses generated by the three GPT models and assessing the quality of the response generated.

![AI-Generated Image of *Lycorma delicatula*, by GPT 1](images/Screenshot%202024-04-28%20005852.png){fig-alt="Generated by GPT 1" fig-align="center" width="319"}

# [Methods]{.underline}

## *Construction of Knowledge Base*

For the GPT 1 model, the knowledge base that was constructed composes of scientific research papers, guidelines, and open source website about Spotted Lanternfly management. To webscrape a website, the R package, rvest [@rvest], was used. The knowledge base contains webscraped articles from Penn State's Spotted Lanternfly website.

```{r}
#| label: web_scraping
#| results: hide

# Example of Webscraping Spotted Lanternfly Management Guide:

# Assigning Penn State's Spotted Lanternfly Website to an object

library(rvest)
link <-"https://extension.psu.edu/spotted-lanternfly"
PSU_SLF <- rvest::read_html(link)


# Article 1: Spotted Lanternfly Management Guide -------------------------------

# Starting on first article: 
article1 <- "https://extension.psu.edu/spotted-lanternfly-management-guide"
management_guide <- read_html(article1)

# Extracting the article's whole contents:

article1_text <- html_text(management_guide)

# Article's title: Spotted Lanternfly Management Guide
article1_title_text <- management_guide |> 
  html_nodes(".base") |> 
  html_text()

# Article's paragraphs:

article1_paragraph_text <- 
  management_guide |> 
  html_nodes(".value") |> 
  html_text()
```

## *Data*

The data was collected by generating around 30 various questions regarding spotted lanternflies research. Questions included the history, biology, appearance, management, and economical and ecological impacts. The questions were then inputted into the three different GPT models, and the responses were recorded.
The data was then explored using packages: quanteda [@quanteda], quanteda.textplots [@quanteda.textplots], quanteda.textstats [@quanteda.textstats], and tidyverse [@tidyverse]. The figures that were made were word clouds for each GPT models, frequency distribution of words displaying the most common words used per model's response, average response lengths per GPT model, and a similarity analysis of GPT 1 model and GPT 2 model using Cosine Similarity.

# [Results]{.underline}

## Word Clouds

Word clouds are a visual representation of textual data, as it highlights clusters of words that are repeatedly used. In this case, the word clouds of GPT 1, 2, and 3, were created to visualize the difference between commonly used words generated within responses.

### *Word Cloud of GPT 1*

```{r}
#| label: wordcloud_GPT_1
#| fig-cap: Word Cloud of GPT 1 Responses
#| fig-show: animate
#| message: false

library(quanteda)
library(quanteda.textplots)
library(tidyverse)

# Filtering GPT 1's data
GPT1_data <-
  data |> 
  select(Question, Model, Response) |> 
  filter(
    Model == "GPT 1"
  )

# Creating GPT 1's Corpus of Responses:
GPT1_corpus <-
  corpus(GPT1_data$Response) |> 
  tokens(remove_punct = TRUE) |> # creates tokens which are units of text
  tokens_remove(pattern = stopwords(language = "english")) |> 
  dfm()

# Generating Word Cloud
wordcloud_GPT1 <- 
  GPT1_corpus |> 
  textplot_wordcloud(
    min_count = 5,
    color = c('brown', 'red','orange','blue','green','darkgreen'))
```

The figure above portrays the most common words from GPT 1 model's generated responses. The text in dark green represents the most commonly used words in GPT 1's responses which are: spotted and can. The text highlighted in light green represents the second most used word which is lanternflies, while blue represents third most used word: lanternfly. The sizes also contributes to the frequency of the words, the bigger the text is, the greater the frequency it is used.

### *Word Cloud of GPT 2*

```{r}
#| label: wordcloud_GPT_2
#| fig-cap: Word Cloud of GPT 2s Responses
#| fig-show: animate

library(quanteda)
library(quanteda.textplots)
library(tidyverse)

# Filtering GPT 2's data
GPT2_data <-
  data |> 
  select(Question, Model, Response) |> 
  filter(
    Model == "GPT 2"
  )

# Creating GPT 2's Corpus
GPT2_corpus <-
  corpus(GPT2_data$Response) |> 
  tokens(remove_punct = TRUE) |> 
  tokens_remove(pattern = stopwords(language = "english")) |> 
  dfm()

# Generating Word Cloud
wordcloud_GPT2 <- 
  GPT2_corpus |> 
  textplot_wordcloud(
    min_count = 5,
    color = c('brown', 'red','orange','blue','green','darkgreen'))
```

Similarly, the figure above portrays the most common words from GPT 2 model's generated responses. However, the words used in GPT 2 model's responses are similar to GPT 1 model's responses. Also, the amount of words used in GPT 2 model's responses are less than the words used in GPT 1 model's response.

### *Word Cloud of GPT 3*

```{r}
#| label: wordcloud_GPT_3
#| fig-cap: Word Cloud of GPT 3 Responses
#| fig-show: animate

library(quanteda)
library(quanteda.textplots)
library(tidyverse)

# Filtering for GPT 3's data
GPT3_data <-
  data |> 
  select(Question, Model, Response) |> 
  filter(
    Model == "GPT 3"
  )

# Creating Corpus
GPT3_corpus <-
  corpus(GPT3_data$Response) |> 
  tokens(remove_punct = TRUE) |> 
  tokens_remove(pattern = stopwords(language = "english")) |> 
  dfm()

# Generating Word Cloud
wordcloud_GPT3 <- 
  GPT3_corpus |>  
  textplot_wordcloud(
    min_count = 5,
    color = c('brown', 'red','orange','blue','green','darkgreen'))
```

For GPT 3, it is significantly different from both GPT 1 and GPT 2. Although it has similar commonly used words (such as spotted, lanternflies, and lanternfly), the appearance of the words generated by GPT 3 occurs less often compared to GPT 1 and GPT 2.

## Frequency Distribution of GPT Responses

Frequency distribution visualizes how often words appear in a textual response. As both GPT models have a focus in spotted lanternfly research, the presence of knowledge base can either improve the verbiage used in responses, or have no impact at all.

### *Frequency Distribution of GPT 1*

```{r}
#| label: frequency_plot_GPT1
#| fig-cap: Frequency Distribution of GPT 1 responses

library(quanteda.textstats)
library(tidyverse)

# Calculating GPT 1's most frequent words in responses
tstat_freq_gpt1 <- textstat_frequency(GPT1_corpus, n = 40)

# Visualizing Frequency Distribution

tstat_freq_gpt1 |> 
  ggplot(aes(x = reorder(feature, frequency), y = frequency, fill = frequency)) +
  geom_bar(stat = "identity") +
  coord_flip() + 
    labs(
      title = "Frequency Distribution of Words",
      x = "Feature", 
      y = "Frequency",
      fill = "Frequency"
      ) +
  scale_fill_gradient(low = "black", high = "red") +
  theme_light()
```

The figure portrays the frequency of the words generated in GPT 1's knowledge base. The most frequent word is "spotted", occurring at `r subset(tstat_freq_gpt1, feature == "spotted")$frequency` times. The least frequent word is "potential", occurring at `r subset(tstat_freq_gpt1, feature == "potential")$frequency` times.

### *Frequency Distribution of GPT 2*

```{r}
#| label: frequency_plot_GPT2
#| fig-cap: Frequency Distribution of GPT 2 responses

library(quanteda.textstats)
library(tidyverse)

# Calculating GPT 2's most frequent words in responses
tstat_freq_gpt2 <- textstat_frequency(GPT2_corpus, n = 40)

# Visualizing Frequency Distribution
tstat_freq_gpt2 |> 
  ggplot(aes(x = reorder(feature, frequency), y = frequency, fill = frequency)) +
  geom_bar(stat = "identity") +
  coord_flip() + 
    labs(
      title = "Frequency Distribution of Words",
      x = "Feature", 
      y = "Frequency",
      fill = "Frequency"
      ) +
  scale_fill_gradient(low = "black", high = "red") + 
  theme_light()
```

The most frequent word for GPT 2 is "can", occurring at `r subset(tstat_freq_gpt2, feature == "can")$frequency` times, while "spotted" occurs `r subset(tstat_freq_gpt2, feature == "spotted")$frequency` times. The least frequent word is "early", occurring at `r subset(tstat_freq_gpt2, feature == "early")$frequency` times.

### *Frequency Distribution of GPT 3*

```{r}
#| label: frequency_plot_GPT3
#| fig-cap: Frequency Distribution of GPT 3 responses

library(quanteda.textstats)
library(tidyverse)

# Calculating GPT 3's most frequent words in responses

tstat_freq_gpt3 <- textstat_frequency(GPT3_corpus, n = 40)

# Visualizing Frequency Distribution

tstat_freq_gpt3 |> 
  ggplot(aes(x = reorder(feature, frequency), y = frequency, fill = frequency)) +
  geom_bar(stat = "identity") +
  coord_flip() + 
    labs(
      title = "Frequency Distribution of Words",
      x = "Feature", 
      y = "Frequency",
      fill = "Frequency"
      ) +
  scale_fill_gradient(low = "black", high = "red") +
theme_light()

```

For GPT 3's frequency distribution, the most frequent word is "spotted", occurring at `r subset(tstat_freq_gpt3, feature == "spotted")$frequency` times. The least frequent word is "dispersal", occurring at `r subset(tstat_freq_gpt3, feature == "dispersal")$frequency` times.

## Average Response Length Per GPT 1, 2, and 3

Average response length portrays the length of the GPT responses, as it is measured by the number of characters per string.

```{r}
#| label: average_responses
#| fig-cap: Average Lengths of Responses per GPT

library(tidyverse)

# Calculating Average Length of responses per GPT
data_lengths <- 
  data |> 
  group_by(Model) |> 
  summarize(average_length = mean(nchar(Response)))

# Visualizing Average Response Lengths

ggplot(data_lengths, aes(x = Model, y = average_length, fill = Model)) +
  geom_bar(stat = "identity") +
  labs(title = "Average Length of GPT Responses",
       x = "Models",
       y = "Average Response Length",
       fill = "Model Types") +
  scale_fill_manual(values = c("red", "brown", "black")) +
  theme_minimal()
```

The figure portrays the average length of each model's responses. For GPT 1, the average length is `r subset(data_lengths, Model == "GPT 1")$average_length` characters. For GPT 2, the average length is `r subset(data_lengths, Model == "GPT 2")$average_length` characters, while the average length for GPT 3 is `r subset(data_lengths, Model == "GPT 3")$average_length` characters. This shows that GPT 1, containing the knowledge base, provides more detailed responses, compared to GPT 3. However, for GPT 2, the number of characters is almost equivalent to GPT 1, except that GPT 2 uses less characters for each response generated, possibly providing concise responses.

## Visualizing Similarities Between GPT 1 and GPT 2

Since GPT 1 and GPT 2 are focused on spotted lanternfly research, the similarity of responses generated by both models to the same inquiries are visualized. To visualize the similarity between GPT 1 and GPT 2's responses, the similarity scores were calculated using cosine similarity.

Cosine similarity scores are a measure of how similar two objects are, in terms of their content and context, based on the angle (cosine) between their vector representations in a multi-dimensional space. The vectors represent the frequency (or weight) of terms in each document, with each dimension corresponding to a different term.

To calculate cosine similarity, the corpora of GPT 1 and GPT 2 are transformed into document feature matrices (DFM). DFM are used to organize textual data into a matrix where rows represent documents and columns represent features (such as words or terms).

```{r}
#| label: similarity_GPT1_vs_GPT2
#| fig-cap: Figure represents similarity score between GPT 1 and GPT 2

library(quanteda)
library(quanteda.textstats)
library(tidyverse)

# Creating DFM for GPT 1 and GPT 2:
GPT1_corpus <-
  corpus(GPT1_data$Response) |> 
  tokens(remove_punct = TRUE) |> 
  tokens_remove(pattern = stopwords(language = "english")) 

# DFM for GPT 1
dfmat_gpt1 <- dfm(GPT1_corpus)

GPT2_corpus <-
  corpus(GPT2_data$Response) |> 
  tokens(remove_punct = TRUE) |> 
  tokens_remove(pattern = stopwords(language = "english")) 

# DFM for GPT 2
dfmat_gpt2 <- dfm(GPT2_corpus)

# Creating tibbles for GPT 1 features and GPT 2 features:
features_gpt1 <- tibble(feature = featnames(dfmat_gpt1))
features_gpt2 <- tibble(feature = featnames(dfmat_gpt2))

# Combining tibbles
all_features <- bind_rows(features_gpt1, features_gpt2) |> 
                distinct() |> 
                pull(feature)

# Aligning GPT 1 and 2's DFM to have matching features (size):
dfmat_gpt1_aligned <- dfm_match(dfmat_gpt1, all_features)
dfmat_gpt2_aligned <- dfm_match(dfmat_gpt2, all_features)

# Calculating similarity scores between GPT 1 DFM and GPT 2 DFM using Cosine Similarity:

similarity_scores <- textstat_simil(dfmat_gpt1_aligned, dfmat_gpt2_aligned, method = "cosine", margin = "documents")

# Creating tibble of similarity scores between GPT 1 and GPT 2:
similarity_data <- tibble(Similarity = diag(as.matrix(similarity_scores)))

# Visualize the similarity scores:
similarity_data |> 
  ggplot(aes(x = Similarity)) +
  geom_histogram(binwidth = 0.05, fill = "red", color = "black") +
  labs(
    title = "Distribution of Cosine Similarity Scores",
    subtitle = "Scores are assigned between GPT 1 and GPT 2 Responses",
    x = "Cosine Similarity Score", 
    y = "Occurrences of Similar Features in Responses") +
  theme_minimal()
```

From the figure, the cosine similarity score ranges from 0 to 1. 0 signifies no similarity between the responses while 1 means identical responses to each questions asked. Majority of scores are concentrated around the 0.75, which portrays that many responses from GPT 1 and GPT 2 are fairly similar. There is a smaller peak around 0.50, indicating a moderate level of similarity for a number of responses. Additionally, low scores (around 0.25) and high scores (around 1.00) are less frequent, which may be due to few instances where responses are either very different or identical. The distribution portrays that while GPT 1 and GPT 2 generate responses that are generally similar, there are possible variations in wording, detail, or perspective that lead to variations in similarity scores.

# [Discussion]{.underline}

From the results shown, the presence of a knowledge base slightly improves the responses of a GPT, but does not significantly impact the performance or quality of a GPT. When comparing the word clouds of each GPT model, GPT 1 and GPT 2 had similar common words. The frequency distributions per model portrays that the presence of a knowledge base has minimal impact in the generated responses. With the average lengths per response, it shows that GPT 1 had the most characters used in responses, which shows that GPT 1 provides detailed but not concise responses. Lastly, the similarity scores between GPT 1 and GPT 2 show that the responses between GPT 1 and GPT 2 are similar for some questions, but different for others.

This shows the pattern that the presence of a knowledge base has a minimal impact on the responses generated by GPT models. This work will benefit Generative AI engineers, researchers utilizing AI, and developers, as they could fine-tune GPT models with a knowledge base. Conducting more research about the significance of a knowledge base and prompt engineering would greatly benefit the future of Generative AI by understanding the factors that influence response quality and performance of GPT. Further exploration within the realm of knowledge bases and AI prompt engineering could help elucidate whether different knowledge bases or integration techniques yield different results.

# [References]{.underline}

1.  Fui-Hoon Nah, Fiona, Ruilin Zheng, Jingyuan Cai, Keng Siau, and LangtaoChen. 2023. âGenerative AI and ChatGPT: Applications, Challenges,and AI-Human Collaboration.â *Journal of InformationTechnology Case and Application Research* 25 (3): 277â304. <https://doi.org/10.1080/15228053.2023.2233814>
2.  Ooi, Keng-Boon, Garry Wei-Han Tan, Mostafa Al-Emran, Mohammed A.Al-Sharafi, Alexandru Capatina, Amrita Chakraborty, Yogesh K. Dwivedi,et al. 2023. âThe Potential of Generative Artificial IntelligenceAcross Disciplines: Perspectives and Future Directions.â*Journal of Computer Information Systems*, October, 1â32. <https://doi.org/10.1080/08874417.2023.2261010>
3.  Urban, Julie M., and Heather Leach. 2023. âBiology and Managementof the Spotted Lanternfly, *Lycorma Delicatula* (Hemiptera:Fulgoridae), in the United States.â *Annual Review of Entomology* 68 (1): 151â67. <https://doi.org/10.1146/annurev-ento-120220-111140>.

*Packages Used*:

1.  Benoit, Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng,Stefan MÃ¼ller, and Akitaka Matsuo. 2018. âQuanteda: An r Packagefor the Quantitative Analysis of Textual Dataâ 3: 774. <https://doi.org/10.21105/joss.00774>.
2.  Iannone, Richard, Joe Cheng, Barret Schloerke, Ellis Hughes, AlexandraLauer, and JooYoung Seo. 2024. âGt: Easily CreatePresentation-Ready Display Tables.â [https://CRAN.R-project.org/package=gt](https://cran.r-project.org/package=gt).
3.  Wickham, Hadley. 2024. âRvest: Easily Harvest (Scrape) WebPages.â [https://CRAN.R-project.org/package=rvest](https://cran.r-project.org/package=rvest).
4.  Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, LucyDâAgostino McGowan, Romain FranÃ§ois, Garrett Grolemund, et al. 2019.âWelcome to the Tidyverseâ 4: 1686. <https://doi.org/10.21105/joss.01686>.
